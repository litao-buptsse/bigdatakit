#!/bin/bash

print_usage() {
cat <<EOF
Usage: $0 <command> [-options] [args...]

commands:
  spark-streaming [-options] <jar>           submit a Spark Streaming job

  create-table [-options] <table spec file>  create hive table

  hive-etl [-options] <jar> <logdate>        submit a hive etl job
  hbase-etl [-options] <jar> <logdate>       submit a hbase etl job
  phoenix-etl [-options] <jar> <logdate>     submit a phoenix etl job

  generate-project <project name>            generate base project structure
  publish-package <package name> <jar>       publish user defined package with group:artifact:version name format

  spark-submit [spark options]               execute spark-submit command
  spark-shell [spark options]                execute spark-shell command
  spark-sql [spark options]                  execute spark-sql command
  presto [presto options]                    execute presto client
  phoenix [phoenix options]                  execute phoenix client

  version                                    display version
  upgrade [version]                          upgrade bigdatakit include maven, rpm, docker
  help                                       display this help text


general options:
  -Dpackages=<package list> specify comma-separated list, like group1:artifact1:version1,group2:artifact2:version2

spark-streaming options:
  -Dmaster=<url>            specify the Spark run mode, "local[*]" or "yarn-client"
  -Dname=<name>             specify the name of Spark application
  -Dapproach=<approach>     specify the Kafka approach, "receiver-based" or "direct-based"
  -DzkConnString=<url>      specify the ZooKeeper connection to use
  -Dtopics=<topics>         specify the Kafka topics with comma seperated
  -DgroupId=<group>         specify the Kafka consumer group id
  -DbatchDuration=<seconds> specify the Spark Streaming batch process duration
  -Dprocessor=<class>       specify the processor class
  -DthreadNum=<num>         specify the Kafka consumer thread num (only need for "receiver-based")
  -DbrokerList=<url>        specify the kafka broker list (only need for "direct-based")
  -DoffsetsCommitBatchInterval=<interval>  specify the Kafka offsets commit batch interval  (only need for "direct-based")
  -D<property>=<value>      specify any Java system property value

hive-etl options:
  -Dmaster=<url>            specify the Spark run mode, "local[*]" or "yarn-client"
  -Dname=<name>             specify the name of Spark application
  -Ddatabase=<database>     specify the database, default custom
  -Dtable=<table>           specify the table
  -Dprocessor=<class>       specify the processor class
  -Dparallelism=<num>       specify the parallelism which will influence orc file num and write parallelism, default 1

hbase-etl options:
  -Dmaster=<url>            specify the Spark run mode, "local[*]" or "yarn-client"
  -Dname=<name>             specify the name of Spark application
  -Dnamespace=<namespace>   specify the database, default default
  -Dtable=<table>           specify the table
  -DcolumnFamily=<columnFamily>  specify the DcolumnFamily
  -Dprocessor=<class>       specify the processor class
  -Dapproach=<approach>     specify the hbase import approach, "put" or "bulkload"
  -Dparallelism=<num>       specify the parallelism which will influence orc file num and write parallelism, default 1

phoenix-etl options:
  -Dmaster=<url>            specify the Spark run mode, "local[*]" or "yarn-client"
  -Dname=<name>             specify the name of Spark application
  -Dtable=<table>           specify the table
  -Dprocessor=<class>       specify the processor class
  -Dparallelism=<num>       specify the parallelism which will influence orc file num and write parallelism, default 1

EOF
}

submit_spark_streaming() {
  userJar=$ARGS
  USER_JAR=`get_absolute_path $userJar`
  if [ $? -ne 0 ]; then
    echo "$userJar is not existed"
    return 1
  fi

  LOCAL_CLASSPATH=$SPARK_CLASSPATH:$SPARK_CONF_DIR:$SPARK_ASSEMBLY_JAR:$SPARK_DATAUNCLEUS_CLASSPATH:$BIGDATAKIT_CONF_DIR:$BIGDATAKIT_CLASSPATH:$USER_JAR:$PACKAGES_CLASSPATH
  SPARK_JARS=`classpath_to_sparkjars $BIGDATAKIT_CLASSPATH:$USER_JAR:$PACKAGES_CLASSPATH`
  JAVA_OPTS=()
  for opt in "${OPTIONS[@]}"; do
    newopt=`echo $opt | sed "s/-D/-Droot.sparkStreaming./"`
    JAVA_OPTS+=("$newopt")
  done
  MAIN_CLASS="com.sogou.bigdatakit.streaming.SparkStreaming"

  $JAVA_HOME/bin/java -Djava.library.path=$LD_LIBRARY_PATH "${JAVA_OPTS[@]}" -Dspark.jars=$SPARK_JARS -cp $LOCAL_CLASSPATH $MAIN_CLASS
}

create_table() {
  LOCAL_CLASSPATH=$SPARK_CLASSPATH:$SPARK_CONF_DIR:$SPARK_ASSEMBLY_JAR:$SPARK_DATAUNCLEUS_CLASSPATH:$BIGDATAKIT_CONF_DIR:$BIGDATAKIT_CLASSPATH:$HIVE_AUXLIB_CLASSPATH:$PACKAGES_CLASSPATH
  JAVA_OPTS=()
  for opt in "${OPTIONS[@]}"; do
    JAVA_OPTS+=("$opt")
  done
  MAIN_CLASS="com.sogou.bigdatakit.hive.CreateTable"

  $JAVA_HOME/bin/java -Djava.library.path=$LD_LIBRARY_PATH -XX:MaxPermSize=256m "${JAVA_OPTS[@]}" -cp $LOCAL_CLASSPATH $MAIN_CLASS $ARGS
}

hive-etl() {
  userJar=`echo $ARGS | awk '{print $1}'`
  USER_JAR=`get_absolute_path $userJar`
  if [ $? -ne 0 ]; then
    echo "$userJar is not existed"
    return 1
  fi
  if [ $ARGS_NUM -eq 2 ]; then
    logdate=`echo $ARGS| awk '{print $2}'`
  elif [ $ARGS_NUM -eq 3 ]; then
    logid=`echo $ARGS| awk '{print $2}'`
    logdate=`echo $ARGS| awk '{print $3}'`
  fi

  LOCAL_CLASSPATH=$SPARK_CLASSPATH:$SPARK_CONF_DIR:$SPARK_ASSEMBLY_JAR:$SPARK_DATAUNCLEUS_CLASSPATH:$BIGDATAKIT_CONF_DIR:$BIGDATAKIT_CLASSPATH:$USER_JAR:$HIVE_AUXLIB_CLASSPATH:$PACKAGES_CLASSPATH
  SPARK_JARS=`classpath_to_sparkjars $BIGDATAKIT_CLASSPATH:$USER_JAR:$HIVE_AUXLIB_CLASSPATH:$PACKAGES_CLASSPATH`
  JAVA_OPTS=()
  for opt in "${OPTIONS[@]}"; do
    newopt=`echo $opt | sed "s/-D/-Droot.etl.hive./"`
    JAVA_OPTS+=("$newopt")
  done
  MAIN_CLASS="com.sogou.bigdatakit.etl.hive.HiveETL"

  $JAVA_HOME/bin/java -Djava.library.path=$LD_LIBRARY_PATH -Xmx1G -XX:MaxPermSize=256M "${JAVA_OPTS[@]}" -Dspark.jars=$SPARK_JARS -cp $LOCAL_CLASSPATH $MAIN_CLASS $logdate
}

hbase-etl() {
  userJar=`echo $ARGS | awk '{print $1}'`
  USER_JAR=`get_absolute_path $userJar`
  if [ $? -ne 0 ]; then
    echo "$userJar is not existed"
    return 1
  fi
  if [ $ARGS_NUM -eq 2 ]; then
    logdate=`echo $ARGS| awk '{print $2}'`
  elif [ $ARGS_NUM -eq 3 ]; then
    logid=`echo $ARGS| awk '{print $2}'`
    logdate=`echo $ARGS| awk '{print $3}'`
  fi

  LOCAL_CLASSPATH=$SPARK_CLASSPATH:$SPARK_CONF_DIR:$SPARK_ASSEMBLY_JAR:$SPARK_DATAUNCLEUS_CLASSPATH:$BIGDATAKIT_CONF_DIR:$BIGDATAKIT_CLASSPATH:$USER_JAR:$HIVE_AUXLIB_CLASSPATH:$PACKAGES_CLASSPATH
  SPARK_JARS=`classpath_to_sparkjars $BIGDATAKIT_CLASSPATH:$USER_JAR:$HIVE_AUXLIB_CLASSPATH:$PACKAGES_CLASSPATH`
  JAVA_OPTS=()
  for opt in "${OPTIONS[@]}"; do
    newopt=`echo $opt | sed "s/-D/-Droot.etl.hbase./"`
    JAVA_OPTS+=("$newopt")
  done
  MAIN_CLASS="com.sogou.bigdatakit.etl.hbase.HBaseETL"

  $JAVA_HOME/bin/java -Djava.library.path=$LD_LIBRARY_PATH -Xmx1G -XX:MaxPermSize=256M "${JAVA_OPTS[@]}" -Dspark.jars=$SPARK_JARS -cp $LOCAL_CLASSPATH $MAIN_CLASS $logdate
}

phoenix-etl() {
  userJar=`echo $ARGS | awk '{print $1}'`
  USER_JAR=`get_absolute_path $userJar`
  if [ $? -ne 0 ]; then
    echo "$userJar is not existed"
    return 1
  fi
  if [ $ARGS_NUM -eq 2 ]; then
    logdate=`echo $ARGS| awk '{print $2}'`
  elif [ $ARGS_NUM -eq 3 ]; then
    logid=`echo $ARGS| awk '{print $2}'`
    logdate=`echo $ARGS| awk '{print $3}'`
  fi

  LOCAL_CLASSPATH=$SPARK_CLASSPATH:$SPARK_CONF_DIR:$SPARK_ASSEMBLY_JAR:$SPARK_DATAUNCLEUS_CLASSPATH:$BIGDATAKIT_CONF_DIR:$BIGDATAKIT_CLASSPATH:$USER_JAR:$HIVE_AUXLIB_CLASSPATH:$PACKAGES_CLASSPATH:$PHOENIX_SERVER_JAR:$PHOENIX_SPARK_JAR
  SPARK_JARS=`classpath_to_sparkjars $BIGDATAKIT_CLASSPATH:$USER_JAR:$HIVE_AUXLIB_CLASSPATH:$PACKAGES_CLASSPATH:$PHOENIX_SERVER_JAR:$PHOENIX_SPARK_JAR`
  JAVA_OPTS=()
  for opt in "${OPTIONS[@]}"; do
    newopt=`echo $opt | sed "s/-D/-Droot.etl.phoenix./"`
    JAVA_OPTS+=("$newopt")
  done
  MAIN_CLASS="com.sogou.bigdatakit.etl.phoenix.PhoenixETL"

  $JAVA_HOME/bin/java -Djava.library.path=$LD_LIBRARY_PATH -Xmx1G -XX:MaxPermSize=256M "${JAVA_OPTS[@]}" -Dspark.jars=$SPARK_JARS -cp $LOCAL_CLASSPATH $MAIN_CLASS $logdate
}

generate_project() {
  name=`echo $ARGS| awk '{print $1}'`
  git clone $BIGDATAKIT_STARTUP_GIT_URL $name
  sed -i "s/\${artifactId}/$name/g" $name/pom.xml
  rm -fr $name/.git
}

publish_package() {
  name=`echo $ARGS| awk '{print $1}'`
  groupId=`echo $name | awk -F":" '{print $1}'`
  artifactId=`echo $name | awk -F":" '{print $2}'`
  version=`echo $name | awk -F":" '{print $3}'`
  if [ X$groupId == X ] || [ X$artifactId == X ] || [ X$version == X ]; then
    echo "groupId, artifactId or version is empty"
    return 1
  fi
  userJar=`echo $ARGS | awk '{print $2}'`
  USER_JAR=`get_absolute_path $userJar`
  if [ $? -ne 0 ]; then
    echo "$userJar is not existed"
    return 1
  fi

  HDFS_PACKAGE_DIR=$BIGDATAKIT_HDFS_PACKAGE_DIR/$groupId/$artifactId/$version
  HDFS_PACKAGE_JAR=$HDFS_PACKAGE_DIR/$artifactId-$version.jar
  hadoop fs -mkdir -p $HDFS_PACKAGE_DIR
  hadoop fs -rm $HDFS_PACKAGE_JAR
  echo "try to upload $groupId:$artifactId:$version"
  hadoop fs -copyFromLocal $USER_JAR $HDFS_PACKAGE_JAR
  hadoop fs -chmod -R +w $HDFS_PACKAGE_DIR
  echo "package $groupId:$artifactId:$version pushlish succeed"
}

spark_submit() {
  SPARK_JARS=`classpath_to_sparkjars $BIGDATAKIT_CLASSPATH:$HIVE_AUXLIB_CLASSPATH:$PACKAGES_CLASSPATH`
  $SPARK_HOME/bin/spark-submit --conf spark.jars=$SPARK_JARS $FULL_PARAMS
}

spark_shell() {
  SPARK_JARS=`classpath_to_sparkjars $BIGDATAKIT_CLASSPATH:$HIVE_AUXLIB_CLASSPATH:$PACKAGES_CLASSPATH`
  $SPARK_HOME/bin/spark-shell --conf spark.jars=$SPARK_JARS $FULL_PARAMS
}

spark_sql() {
  SPARK_JARS=`classpath_to_sparkjars $BIGDATAKIT_CLASSPATH:$HIVE_AUXLIB_CLASSPATH:$PACKAGES_CLASSPATH`
  $SPARK_HOME/bin/spark-sql --conf spark.jars=$SPARK_JARS $FULL_PARAMS
}

presto_cli() {
  export PATH=/usr/lib/jvm/jre-1.8.0-oracle.x86_64/bin:$PATH
  $BIGDATAKIT_HOME/bin/ext/presto \
    --server $PRESTO_COODINATOR_URI \
    --catalog $PRESTO_DEFAULT_CATALOG \
    --schema $PRESTO_DEFAULT_SCHEMA \
    $FULL_PARAMS
}

phoenix_cli() {
  $PHOENIX_HOME/bin/sqlline.py $HBASE_ZOOKEER_QUORUM $FULL_PARAMS
}

upgrade() {
  version=$BIGDATAKIT_VERSION
  if [ $ARGS_NUM -eq 1 ]; then
    version=`echo $ARGS | awk '{print $1}'`
  fi

  yum erase bigdatakit -y
  yum clean all
  yum install bigdatakit-$version -y

  mavenHomeDir=~/.m2
  if ! [ -z $M2_HOME ]; then
    mavenHomeDir=$M2_HOME
  fi
  rm -fr $mavenHomeDir/repository/com/sogou/bigdatakit

  docker pull registry.docker.dev.sogou-inc.com:5000/clouddev/bigdatakit:$version
}

print_version() {
  echo $BIGDATAKIT_VERSION
}

classpath_to_sparkjars() {
  echo $1 | awk 'BEGIN{RS=":";ORS=","}{if($1!=""){print "file://"$1}}'
}

get_absolute_path() {
  path=$1
  if [[ `ls $path >/dev/null 2>&1; echo $?` -ne 0 ]]; then
    return 1
  fi
  readlink -f $path
}

parse_params() {
  COMMAND=$1; shift
  while (($#)); do
    if [[ "$1" =~ ^"-" ]]; then
      if [[ "$1" =~ ^"-Dpackages=" ]]; then
        PACKAGES=`echo $1 | awk -F"=" '{print $2}'`
      else
        OPTIONS+=("$1")
        FULL_PARAMS="$FULL_PARAMS $1"
      fi
    else
      ARGS="$ARGS $1"
      ARGS_NUM=`expr $ARGS_NUM + 1`
      FULL_PARAMS="$FULL_PARAMS $1"
    fi
    shift
  done
}

download_packages() {
  packages=$1
  echo -n $packages | awk 'BEGIN{RS=","}{print $0}' >.packages_list
  while read name
  do
    groupId=`echo $name | awk -F":" '{print $1}'`
    artifactId=`echo $name | awk -F":" '{print $2}'`
    version=`echo $name | awk -F":" '{print $3}'`
    if [ X$groupId == X ] || [ X$artifactId == X ] || [ X$version == X ]; then
      echo "invalid package format: $name, need to be group:artifact:version"
      continue
    fi
    HDFS_PACKAGE_DIR=$BIGDATAKIT_HDFS_PACKAGE_DIR/$groupId/$artifactId/$version
    HDFS_PACKAGE_JAR=$HDFS_PACKAGE_DIR/$artifactId-$version.jar
    LOCAL_PACKAGE_DIR=$BIGDATAKIT_LOCAL_PACKAGE_DIR/$groupId/$artifactId/$version
    LOCAL_PACKAGE_JAR=$LOCAL_PACKAGE_DIR/$artifactId-$version.jar
    mkdir -p $LOCAL_PACKAGE_DIR
    HDFS_PACKAGE_SIZE=`hadoop fs -ls $HDFS_PACKAGE_JAR 2>/dev/null | awk '{print $5}'`
    LOCAL_PACKAGE_SIZE=`ls -l $LOCAL_PACKAGE_JAR 2>/dev/null | awk '{print $5}'`
    if [ X$HDFS_PACKAGE_SIZE == X ]; then
      echo "$groupId:$artifactId:$version is not found"
      continue
    fi
    if [ X$LOCAL_PACKAGE_SIZE == X ] || [ $LOCAL_PACKAGE_SIZE -ne $HDFS_PACKAGE_SIZE ]; then
      echo "try to download $groupId:$artifactId:$version"
      rm -f $LOCAL_PACKAGE_JAR
      hadoop fs -copyToLocal $HDFS_PACKAGE_JAR $LOCAL_PACKAGE_DIR
      echo "package $groupId:$artifactId:$version download succeed"
    fi
    PACKAGES_CLASSPATH=$PACKAGES_CLASSPATH:$LOCAL_PACKAGE_JAR
  done <.packages_list
  rm -f .packages_list
}

rsync_hive_auxlib() {
  rsync -rvt --delete rsync.datadir.sogou::datacat/auxlib $HIVE_HOME >/dev/null 2>&1
}

dir=`dirname $0`
dir=`cd $dir/..; pwd`
. $dir/conf/bigdatakit-env.sh

COMMAND=""
OPTIONS=()
ARGS=""
ARGS_NUM=0
PACKAGES=""
PACKAGES_CLASSPATH=""
FULL_PARAMS=""
parse_params "$@"
download_packages $PACKAGES
rsync_hive_auxlib

case $COMMAND in
  help)
    print_usage
    exit 0
    ;;

  spark-streaming)
    if [ $ARGS_NUM -ne 1 ]; then
      echo "Usage: $0 spark-streaming [-options] <jar>"
      exit 1
    fi
    submit_spark_streaming
    exit $?
    ;;

  create-table)
    if [ $ARGS_NUM -ne 1 ]; then
      echo "Usage: $0 create-table [-options] <table spec file>"
      exit 1
    fi
    create_table
    exit $?
    ;;

  hive-etl)
    if [ $ARGS_NUM -ne 2 ] && [ $ARGS_NUM -ne 3 ]; then
      echo "Usage: $0 hive-etl [-options] <jar> <logdate>"
      exit 1
    fi
    hive-etl
    exit $?
    ;;

  hbase-etl)
    if [ $ARGS_NUM -ne 2 ] && [ $ARGS_NUM -ne 3 ]; then
      echo "Usage: $0 hbase-etl [-options] <jar> <logdate>"
      exit 1
    fi
    hbase-etl
    exit $?
    ;;

  phoenix-etl)
    if [ $ARGS_NUM -ne 2 ] && [ $ARGS_NUM -ne 3 ]; then
      echo "Usage: $0 phoenix-etl [-options] <jar> <logdate>"
      exit 1
    fi
    phoenix-etl
    exit $?
    ;;

  generate-project)
    if [ $ARGS_NUM -ne 1 ]; then
      echo "Usage: $0 generate-project <project name>"
      exit 1
    fi
    generate_project
    exit $?
    ;;

  publish-package)
    if [ $ARGS_NUM -ne 2 ]; then
      echo "Usage: $0 publish-package <group:artifact:version> <jar>"
      exit 1
    fi
    publish_package
    exit $?
    ;;

  spark-shell)
    shift
    spark_shell $@
    exit $?
    ;;

  spark-submit)
    shift
    spark_submit $@
    exit $?
    ;;

  spark-sql)
    shift
    spark_sql $@
    exit $?
    ;;

  presto)
    shift
    presto_cli $@
    exit $?
    ;;
  
  phoenix)
    shift
    phoenix_cli $@
    exit $?
    ;;

  upgrade)
    shift
    upgrade
    exit $?
    ;;
  
  version)
    shift
    print_version
    exit $?
    ;;

  *)
    print_usage
    exit 1
    ;;
esac
